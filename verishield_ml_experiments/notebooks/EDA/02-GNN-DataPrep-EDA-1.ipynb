{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook: 02-GNN-DataPrep-EDA-1\n",
      "Analyzing processed GNN data in 'default/processed_gnn' directory (no visuals).\n"
     ]
    }
   ],
   "source": [
    "# Jupyter Notebook: 02-GNN-DataPrep-EDA-1.ipynb\n",
    "# =============================================\n",
    "# Purpose:\n",
    "#  - Thoroughly analyze the processed GNN data in 'processed_gnn/' directory.\n",
    "#  - Produce text-based summaries (no plots/visualizations).\n",
    "#  - Validate feature shapes, mask splits, label distributions, etc.\n",
    "#  - Guide further refinement of GNN architecture choices.\n",
    "\n",
    "# =====================================================================\n",
    "# Cell 1: Imports & Initial Config\n",
    "# =====================================================================\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# We use 'print' for text-based outputs; \n",
    "# no matplotlib/seaborn to maintain a purely \"analytical\" text notebook.\n",
    "\n",
    "print(\"Notebook: 02-GNN-DataPrep-EDA-1\")\n",
    "print(\"Analyzing processed GNN data in 'default/processed_gnn' directory (no visuals).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario: default\n",
      "Looking for processed data in: /Users/harshil/Development/GitHub_Repos/VeriShield-AI-Financial-Verification-Platform/verishield_ml_experiments/data_generators/data-huge/default/processed_gnn\n",
      "\n",
      "Checking for required files:\n",
      "  user_features.npy              => FOUND\n",
      "  user_labels.npy                => FOUND\n",
      "  biz_features.npy               => FOUND\n",
      "  biz_labels.npy                 => FOUND\n",
      "  edge_user_user.npy             => FOUND\n",
      "  edge_user_biz.npy              => FOUND\n",
      "  metadata.json                  => FOUND\n",
      "\n",
      "Checking for optional mask files:\n",
      "  train_mask_users.npy           => FOUND\n",
      "  val_mask_users.npy             => FOUND\n",
      "  test_mask_users.npy            => FOUND\n",
      "  train_mask_biz.npy             => MISSING\n",
      "  val_mask_biz.npy               => MISSING\n",
      "  test_mask_biz.npy              => MISSING\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# Cell 2: Define Paths & Check Files\n",
    "# =====================================================================\n",
    "# Adjust these paths according to your local environment if needed.\n",
    "# Typically, we rely on the structure:\n",
    "# verishield_ml_experiments/data_generators/data/<SCENARIO>/processed_gnn/\n",
    "\n",
    "SCENARIO = \"default\"\n",
    "BASE_DIR = (\n",
    "    \"/Users/harshil/Development/GitHub_Repos/\"\n",
    "    \"VeriShield-AI-Financial-Verification-Platform/\"\n",
    "    \"verishield_ml_experiments/data_generators/data-huge\"\n",
    ")\n",
    "PROCESSED_DIR = os.path.join(BASE_DIR, SCENARIO, \"processed_gnn\")\n",
    "\n",
    "print(f\"Scenario: {SCENARIO}\")\n",
    "print(f\"Looking for processed data in: {PROCESSED_DIR}\")\n",
    "\n",
    "# List expected files\n",
    "expected_files = [\n",
    "    \"user_features.npy\",\n",
    "    \"user_labels.npy\",\n",
    "    \"biz_features.npy\",\n",
    "    \"biz_labels.npy\",\n",
    "    \"edge_user_user.npy\",\n",
    "    \"edge_user_biz.npy\",\n",
    "    \"metadata.json\"\n",
    "]\n",
    "\n",
    "# We'll see if train/val/test masks exist (both user & biz).\n",
    "optional_files = [\n",
    "    \"train_mask_users.npy\",\n",
    "    \"val_mask_users.npy\",\n",
    "    \"test_mask_users.npy\",\n",
    "    \"train_mask_biz.npy\",\n",
    "    \"val_mask_biz.npy\",\n",
    "    \"test_mask_biz.npy\"\n",
    "]\n",
    "\n",
    "all_found = True\n",
    "print(\"\\nChecking for required files:\")\n",
    "for ef in expected_files:\n",
    "    path = os.path.join(PROCESSED_DIR, ef)\n",
    "    exists = os.path.isfile(path)\n",
    "    print(f\"  {ef:30s} => {'FOUND' if exists else 'MISSING'}\")\n",
    "    if not exists:\n",
    "        all_found = False\n",
    "\n",
    "print(\"\\nChecking for optional mask files:\")\n",
    "for of in optional_files:\n",
    "    path = os.path.join(PROCESSED_DIR, of)\n",
    "    exists = os.path.isfile(path)\n",
    "    status = \"FOUND\" if exists else \"MISSING\"\n",
    "    print(f\"  {of:30s} => {status}\")\n",
    "\n",
    "if not all_found:\n",
    "    print(\"\\nWARNING: One or more required files are missing. \"\n",
    "          \"This EDA might be incomplete.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded metadata.json contents:\n",
      "  scenario: default\n",
      "  num_users: 453738\n",
      "  num_businesses: 69040\n",
      "  user_feature_cols: ['segment_code', 'is_ring_leader', 'ip_count_log', 'phone_susp', 'email_susp', 'country_watch', 'burst_signup']\n",
      "  biz_feature_cols: ['watchlist_regctry', 'susp_name_flag', 'biz_age_log']\n",
      "  do_split: True\n",
      "  train_ratio: 0.7\n",
      "  val_ratio: 0.15\n",
      "  test_ratio: 0.15\n",
      "  SINGLE_TASK_USER_ONLY: True\n",
      "  edges_user_user_count: 45646\n",
      "  edges_user_biz_count: 996203\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# Cell 3: Load Metadata & Summaries\n",
    "# =====================================================================\n",
    "metadata_path = os.path.join(PROCESSED_DIR, \"metadata.json\")\n",
    "metadata = {}\n",
    "\n",
    "if os.path.isfile(metadata_path):\n",
    "    with open(metadata_path, \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "    print(\"\\nLoaded metadata.json contents:\")\n",
    "    for k, v in metadata.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "else:\n",
    "    print(\"\\nNo metadata.json found; skipping metadata checks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Loaded Core Arrays =====\n",
      "user_features: shape=(453738, 7), dtype=float32\n",
      "user_labels:   shape=(453738,), dtype=int64\n",
      "biz_features:  shape=(69040, 3), dtype=float32\n",
      "biz_labels:    shape=(69040,), dtype=int64\n",
      "edge_user_user: shape=(2, 45646), dtype=int64\n",
      "edge_user_biz:  shape=(2, 996203), dtype=int64\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# Cell 4: Load Core Arrays & Basic Checks\n",
    "# =====================================================================\n",
    "def load_array(filename):\n",
    "    path = os.path.join(PROCESSED_DIR, filename)\n",
    "    if os.path.isfile(path):\n",
    "        return np.load(path)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "user_features = load_array(\"user_features.npy\")\n",
    "user_labels   = load_array(\"user_labels.npy\")\n",
    "biz_features  = load_array(\"biz_features.npy\")\n",
    "biz_labels    = load_array(\"biz_labels.npy\")\n",
    "\n",
    "edge_user_user = load_array(\"edge_user_user.npy\")\n",
    "edge_user_biz  = load_array(\"edge_user_biz.npy\")\n",
    "\n",
    "print(\"\\n===== Loaded Core Arrays =====\")\n",
    "if user_features is not None:\n",
    "    print(f\"user_features: shape={user_features.shape}, dtype={user_features.dtype}\")\n",
    "if user_labels is not None:\n",
    "    print(f\"user_labels:   shape={user_labels.shape}, dtype={user_labels.dtype}\")\n",
    "if biz_features is not None:\n",
    "    print(f\"biz_features:  shape={biz_features.shape}, dtype={biz_features.dtype}\")\n",
    "if biz_labels is not None:\n",
    "    print(f\"biz_labels:    shape={biz_labels.shape}, dtype={biz_labels.dtype}\")\n",
    "\n",
    "if edge_user_user is not None:\n",
    "    print(f\"edge_user_user: shape={edge_user_user.shape}, dtype={edge_user_user.dtype}\")\n",
    "if edge_user_biz is not None:\n",
    "    print(f\"edge_user_biz:  shape={edge_user_biz.shape}, dtype={edge_user_biz.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Label Distribution Checks =====\n",
      "Users => count=453738, fraud_count=98738, fraud_ratio=21.76%\n",
      "Businesses => count=69040, fraud_count=32561, fraud_ratio=47.16%\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# Cell 5: Check Label Distributions\n",
    "# =====================================================================\n",
    "print(\"\\n===== Label Distribution Checks =====\")\n",
    "\n",
    "if user_labels is not None:\n",
    "    num_users = user_labels.shape[0]\n",
    "    user_fraud_count = np.sum(user_labels == 1)\n",
    "    print(f\"Users => count={num_users}, fraud_count={user_fraud_count}, \"\n",
    "          f\"fraud_ratio={user_fraud_count/num_users:.2%}\")\n",
    "\n",
    "if biz_labels is not None:\n",
    "    num_biz = biz_labels.shape[0]\n",
    "    biz_fraud_count = np.sum(biz_labels == 1)\n",
    "    print(f\"Businesses => count={num_biz}, fraud_count={biz_fraud_count}, \"\n",
    "          f\"fraud_ratio={biz_fraud_count/num_biz:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== User Features Stats =====\n",
      "  Column 0: min=0.000, mean=0.411, std=0.696, max=3.000\n",
      "  Column 1: min=0.000, mean=0.005, std=0.071, max=1.000\n",
      "  Column 2: min=0.693, mean=1.596, std=1.807, max=5.394\n",
      "  Column 3: min=0.000, mean=0.268, std=0.443, max=1.000\n",
      "  Column 4: min=0.000, mean=0.000, std=0.000, max=0.000\n",
      "  Column 5: min=0.000, mean=0.030, std=0.171, max=1.000\n",
      "  Column 6: min=0.000, mean=0.050, std=0.218, max=1.000\n",
      "\n",
      "===== Business Features Stats =====\n",
      "  Column 0: min=0.000, mean=0.031, std=0.172, max=1.000\n",
      "  Column 1: min=0.000, mean=0.401, std=0.490, max=1.000\n",
      "  Column 2: min=0.693, mean=7.613, std=0.994, max=8.609\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# Cell 6: Explore Basic Features\n",
    "# =====================================================================\n",
    "print(\"\\n===== User Features Stats =====\")\n",
    "\n",
    "if user_features is not None and user_labels is not None:\n",
    "    # Let's do quick min/mean/max per column\n",
    "    num_user_cols = user_features.shape[1]\n",
    "    for col_idx in range(num_user_cols):\n",
    "        col_data = user_features[:, col_idx]\n",
    "        cmin = np.min(col_data)\n",
    "        cmax = np.max(col_data)\n",
    "        cmean = np.mean(col_data)\n",
    "        cstd = np.std(col_data)\n",
    "        print(f\"  Column {col_idx}: min={cmin:.3f}, mean={cmean:.3f}, std={cstd:.3f}, max={cmax:.3f}\")\n",
    "\n",
    "print(\"\\n===== Business Features Stats =====\")\n",
    "if biz_features is not None and biz_labels is not None:\n",
    "    num_biz_cols = biz_features.shape[1]\n",
    "    for col_idx in range(num_biz_cols):\n",
    "        col_data = biz_features[:, col_idx]\n",
    "        cmin = np.min(col_data)\n",
    "        cmax = np.max(col_data)\n",
    "        cmean = np.mean(col_data)\n",
    "        cstd = np.std(col_data)\n",
    "        print(f\"  Column {col_idx}: min={cmin:.3f}, mean={cmean:.3f}, std={cstd:.3f}, max={cmax:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Edge Analysis =====\n",
      "User-User edges => shape=(2, 45646), total_edges=45646\n",
      "User-Business edges => shape=(2, 996203), total_edges=996203\n",
      "  Distinct users in user-business edges: 181139 out of 453738\n",
      "  Distinct businesses in user-business edges: 69040 out of 69040\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# Cell 7: Edge Analysis\n",
    "# =====================================================================\n",
    "print(\"\\n===== Edge Analysis =====\")\n",
    "if edge_user_user is not None:\n",
    "    euu_shape = edge_user_user.shape\n",
    "    euu_count = euu_shape[1] if len(euu_shape) > 1 else 0\n",
    "    print(f\"User-User edges => shape={euu_shape}, total_edges={euu_count}\")\n",
    "    # Possibly check a quick \"unique\" edge count ignoring duplicates:\n",
    "    # We skip it if we prefer purely text-based quick checks.\n",
    "\n",
    "if edge_user_biz is not None:\n",
    "    eub_shape = edge_user_biz.shape\n",
    "    eub_count = eub_shape[1] if len(eub_shape) > 1 else 0\n",
    "    print(f\"User-Business edges => shape={eub_shape}, total_edges={eub_count}\")\n",
    "\n",
    "# Optionally check how many unique users appear in user-biz edges, etc.\n",
    "if edge_user_biz is not None and user_labels is not None and biz_labels is not None:\n",
    "    # shape(2, E) => row[0]=user_ids, row[1]=biz_ids\n",
    "    user_ids_in_biz = np.unique(edge_user_biz[0,:])\n",
    "    biz_ids_in_biz  = np.unique(edge_user_biz[1,:])\n",
    "    print(f\"  Distinct users in user-business edges: {len(user_ids_in_biz)} out of {user_labels.shape[0]}\")\n",
    "    print(f\"  Distinct businesses in user-business edges: {len(biz_ids_in_biz)} out of {biz_labels.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Checking Train/Val/Test Masks =====\n",
      "Users => train=317616, val=68060, test=68062, total=453738\n",
      "  Sum of splits vs total: 453738 / 453738\n",
      "No business train/val/test masks found or SINGLE_TASK_USER_ONLY is True.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# Cell 8: Mask Splits (If Present)\n",
    "# =====================================================================\n",
    "print(\"\\n===== Checking Train/Val/Test Masks =====\")\n",
    "train_mask_users = load_array(\"train_mask_users.npy\")\n",
    "val_mask_users   = load_array(\"val_mask_users.npy\")\n",
    "test_mask_users  = load_array(\"test_mask_users.npy\")\n",
    "\n",
    "train_mask_biz = load_array(\"train_mask_biz.npy\")\n",
    "val_mask_biz   = load_array(\"val_mask_biz.npy\")\n",
    "test_mask_biz  = load_array(\"test_mask_biz.npy\")\n",
    "\n",
    "def check_mask(mask, label_str):\n",
    "    if mask is None:\n",
    "        print(f\"  {label_str} => MISSING\")\n",
    "        return 0\n",
    "    return mask.sum()\n",
    "\n",
    "if train_mask_users is not None:\n",
    "    user_train_count = check_mask(train_mask_users, \"train_mask_users\")\n",
    "    user_val_count = check_mask(val_mask_users,   \"val_mask_users\")\n",
    "    user_test_count = check_mask(test_mask_users, \"test_mask_users\")\n",
    "    total_users = user_labels.shape[0] if user_labels is not None else 0\n",
    "    print(f\"Users => train={user_train_count}, val={user_val_count}, test={user_test_count}, total={total_users}\")\n",
    "    if total_users > 0:\n",
    "        sum_split = user_train_count + user_val_count + user_test_count\n",
    "        print(f\"  Sum of splits vs total: {sum_split} / {total_users}\")\n",
    "\n",
    "if train_mask_biz is not None:\n",
    "    biz_train_count = check_mask(train_mask_biz, \"train_mask_biz\")\n",
    "    biz_val_count = check_mask(val_mask_biz,     \"val_mask_biz\")\n",
    "    biz_test_count = check_mask(test_mask_biz,   \"test_mask_biz\")\n",
    "    total_biz = biz_labels.shape[0] if biz_labels is not None else 0\n",
    "    print(f\"Businesses => train={biz_train_count}, val={biz_val_count}, test={biz_test_count}, total={total_biz}\")\n",
    "    if total_biz > 0:\n",
    "        sum_split_biz = biz_train_count + biz_val_count + biz_test_count\n",
    "        print(f\"  Sum of splits vs total: {sum_split_biz} / {total_biz}\")\n",
    "else:\n",
    "    print(\"No business train/val/test masks found or SINGLE_TASK_USER_ONLY is True.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Additional Checks / Observations =====\n",
      "User label ratio in train=21.75%, val=21.75%, test=21.84%\n",
      "\n",
      "No visualizations here—purely text-based summaries. Analysis complete.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# Cell 9: Additional Analytical Checks\n",
    "# =====================================================================\n",
    "print(\"\\n===== Additional Checks / Observations =====\")\n",
    "\n",
    "# 1. Possibly check label distributions *within* train/val/test for users\n",
    "if train_mask_users is not None and user_labels is not None:\n",
    "    # user training fraud ratio\n",
    "    user_labels_train = user_labels[train_mask_users]\n",
    "    ratio_train = np.mean(user_labels_train)\n",
    "    ratio_val   = np.mean(user_labels[val_mask_users])   if val_mask_users   is not None else 0\n",
    "    ratio_test  = np.mean(user_labels[test_mask_users])  if test_mask_users  is not None else 0\n",
    "    print(f\"User label ratio in train={ratio_train:.2%}, val={ratio_val:.2%}, test={ratio_test:.2%}\")\n",
    "\n",
    "# 2. Similarly for businesses if multi-task\n",
    "if train_mask_biz is not None and biz_labels is not None:\n",
    "    biz_labels_train = biz_labels[train_mask_biz]\n",
    "    ratio_train_b = np.mean(biz_labels_train)\n",
    "    ratio_val_b   = np.mean(biz_labels[val_mask_biz])   if val_mask_biz   is not None else 0\n",
    "    ratio_test_b  = np.mean(biz_labels[test_mask_biz])  if test_mask_biz  is not None else 0\n",
    "    print(f\"Business label ratio in train={ratio_train_b:.2%}, val={ratio_val_b:.2%}, test={ratio_test_b:.2%}\")\n",
    "\n",
    "print(\"\\nNo visualizations here—purely text-based summaries. Analysis complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Conclusions & Next Steps ===\n",
      "1) We've confirmed shapes of features/labels/edges. Possibly large user-biz edges if scenario=default.\n",
      "2) Observed label distributions. High business fraud ratio is typical for 'default'.\n",
      "3) Checked mask splits to ensure train/val/test sums match total nodes.\n",
      "\n",
      "Recommended next step: proceed to building a PyG HeteroData object and define a multi-task GNN, if needed.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# Cell 10: Conclusions / Next Steps\n",
    "# =====================================================================\n",
    "print(\"\\n=== Conclusions & Next Steps ===\")\n",
    "print(\"1) We've confirmed shapes of features/labels/edges. Possibly large user-biz edges if scenario=default.\")\n",
    "print(\"2) Observed label distributions. High business fraud ratio is typical for 'default'.\")\n",
    "print(\"3) Checked mask splits to ensure train/val/test sums match total nodes.\")\n",
    "print(\"\\nRecommended next step: proceed to building a PyG HeteroData object and define a multi-task GNN, if needed.\")\n",
    "print(\"Done.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
