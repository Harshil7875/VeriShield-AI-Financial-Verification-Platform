{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook: 01-GNN-DataPrep-2 (Updated). Preparing synthetic data (Users, Businesses, IPs) for GNN modeling.\n"
     ]
    }
   ],
   "source": [
    "# Jupyter Notebook: 01-GNN-DataPrep-2.ipynb\n",
    "# =========================================\n",
    "# This notebook prepares VeriShield synthetic data (including IP nodes) for GNN usage.\n",
    "# By: (Harshil Bhandari / 01-19-2025) - Updated for IP-based expansions\n",
    "\n",
    "# =====================================================================\n",
    "# Cell 1: Imports & Global Settings\n",
    "# =====================================================================\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For saving PyTorch structures, if desired (optional).\n",
    "import torch\n",
    "\n",
    "# If you want to create a PyG 'HeteroData' object, import relevant PyG classes.\n",
    "# from torch_geometric.data import HeteroData\n",
    "# import torch_geometric\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "print(\"Notebook: 01-GNN-DataPrep-2 (Updated). Preparing synthetic data (Users, Businesses, IPs) for GNN modeling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario path: /Users/harshil/Development/GitHub_Repos/VeriShield-AI-Financial-Verification-Platform/verishield_ml_experiments/data_generators/data/medium_fraud\n",
      "Processed output will go to: /Users/harshil/Development/GitHub_Repos/VeriShield-AI-Financial-Verification-Platform/verishield_ml_experiments/data_generators/data/medium_fraud/processed_gnn\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# Cell 2: Configuration\n",
    "# =====================================================================\n",
    "# Choose the scenario subfolder\n",
    "SCENARIO = \"medium_fraud\"\n",
    "\n",
    "# Adjust this path to your actual directory where CSVs are generated:\n",
    "# e.g., verishield_ml_experiments/data_generators/data/<SCENARIO>/\n",
    "# or data if you used a separate folder for large runs\n",
    "DATA_BASE_PATH = (\n",
    "    \"/Users/harshil/Development/GitHub_Repos/\"\n",
    "    \"VeriShield-AI-Financial-Verification-Platform/\"\n",
    "    \"verishield_ml_experiments/data_generators/data\"\n",
    ")\n",
    "\n",
    "# We'll store processed .npy files in a subfolder\n",
    "OUTPUT_SUBFOLDER = \"processed_gnn\"\n",
    "\n",
    "SINGLE_TASK_USER_ONLY = False  # If false, we'll do multi-task (users & biz)\n",
    "DO_SPLIT = True               # Create train/val/test splits\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "scenario_path = os.path.join(DATA_BASE_PATH, SCENARIO)\n",
    "processed_dir = os.path.join(scenario_path, OUTPUT_SUBFOLDER)\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Scenario path: {scenario_path}\")\n",
    "print(f\"Processed output will go to: {processed_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames loaded successfully.\n",
      "\n",
      "Shapes:\n",
      "  Users:         (100000, 18)\n",
      "  Businesses:    (10000, 6)\n",
      "  IP Nodes:      (5000, 2)\n",
      "  User->IP:      (100000, 2)\n",
      "  User->Biz:     (221613, 2)\n",
      "  User->User:    (5137, 2)\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# Cell 3: Load CSVs (Now includes IP nodes + user_ip rel)\n",
    "# =====================================================================\n",
    "try:\n",
    "    df_users = pd.read_csv(os.path.join(scenario_path, \"synthetic_users.csv\"))\n",
    "    df_businesses = pd.read_csv(os.path.join(scenario_path, \"synthetic_businesses.csv\"))\n",
    "    df_ip_nodes = pd.read_csv(os.path.join(scenario_path, \"ip_nodes.csv\"))\n",
    "    df_user_ip = pd.read_csv(os.path.join(scenario_path, \"user_ip_relationships.csv\"))\n",
    "    df_user_biz = pd.read_csv(os.path.join(scenario_path, \"user_business_relationships.csv\"))\n",
    "    df_user_user = pd.read_csv(os.path.join(scenario_path, \"user_user_relationships.csv\"))\n",
    "    print(\"DataFrames loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Could not load one of the CSVs. {e}\")\n",
    "    sys.exit(\"Check your scenario folder or file paths.\")\n",
    "\n",
    "print(f\"\\nShapes:\")\n",
    "print(f\"  Users:         {df_users.shape}\")\n",
    "print(f\"  Businesses:    {df_businesses.shape}\")\n",
    "print(f\"  IP Nodes:      {df_ip_nodes.shape}\")\n",
    "print(f\"  User->IP:      {df_user_ip.shape}\")\n",
    "print(f\"  User->Biz:     {df_user_biz.shape}\")\n",
    "print(f\"  User->User:    {df_user_user.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basic ID validation done.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# Cell 4: Basic Validation & Checks (Now includes IP)\n",
    "# =====================================================================\n",
    "expected_users = len(df_users)\n",
    "expected_biz = len(df_businesses)\n",
    "expected_ips = len(df_ip_nodes)\n",
    "\n",
    "max_user_id = df_users[\"user_id\"].max()\n",
    "if max_user_id != expected_users:\n",
    "    print(f\"Warning: max user_id={max_user_id}, but we have {expected_users} user rows. \"\n",
    "          f\"Possible different indexing from the generator?\")\n",
    "\n",
    "max_biz_id = df_businesses[\"business_id\"].max()\n",
    "if max_biz_id != expected_biz:\n",
    "    print(f\"Warning: max business_id={max_biz_id}, but we have {expected_biz} biz rows.\")\n",
    "\n",
    "max_ip_id = df_ip_nodes[\"ip_id\"].max()\n",
    "if max_ip_id != expected_ips:\n",
    "    print(f\"Warning: max ip_id={max_ip_id}, but we have {expected_ips} IP rows.\")\n",
    "\n",
    "# --- Validate user_user edges\n",
    "bad_uids_uu = df_user_user[\n",
    "    (df_user_user['from_user_id'] < 1) | (df_user_user['from_user_id'] > expected_users) |\n",
    "    (df_user_user['to_user_id']   < 1) | (df_user_user['to_user_id']   > expected_users)\n",
    "]\n",
    "if len(bad_uids_uu) > 0:\n",
    "    print(f\"Found {len(bad_uids_uu)} out-of-range user_user edges. Dropping them.\")\n",
    "    df_user_user = df_user_user.drop(bad_uids_uu.index)\n",
    "\n",
    "# --- Validate user_biz edges\n",
    "bad_uids_ub = df_user_biz[\n",
    "    (df_user_biz['user_id'] < 1) | (df_user_biz['user_id'] > expected_users)\n",
    "]\n",
    "bad_bids_ub = df_user_biz[\n",
    "    (df_user_biz['business_id'] < 1) | (df_user_biz['business_id'] > expected_biz)\n",
    "]\n",
    "if len(bad_uids_ub) > 0:\n",
    "    print(f\"Found {len(bad_uids_ub)} out-of-range user IDs in user_biz. Dropping them.\")\n",
    "    df_user_biz = df_user_biz.drop(bad_uids_ub.index)\n",
    "if len(bad_bids_ub) > 0:\n",
    "    print(f\"Found {len(bad_bids_ub)} out-of-range biz IDs in user_biz. Dropping them.\")\n",
    "    df_user_biz = df_user_biz.drop(bad_bids_ub.index)\n",
    "\n",
    "# --- Validate user_ip edges\n",
    "bad_uids_ui = df_user_ip[\n",
    "    (df_user_ip['user_id'] < 1) | (df_user_ip['user_id'] > expected_users)\n",
    "]\n",
    "bad_ipids_ui = df_user_ip[\n",
    "    (df_user_ip['ip_id'] < 1) | (df_user_ip['ip_id'] > expected_ips)\n",
    "]\n",
    "if len(bad_uids_ui) > 0:\n",
    "    print(f\"Found {len(bad_uids_ui)} out-of-range user IDs in user_ip. Dropping them.\")\n",
    "    df_user_ip = df_user_ip.drop(bad_uids_ui.index)\n",
    "if len(bad_ipids_ui) > 0:\n",
    "    print(f\"Found {len(bad_ipids_ui)} out-of-range ip IDs in user_ip. Dropping them.\")\n",
    "    df_user_ip = df_user_ip.drop(bad_ipids_ui.index)\n",
    "\n",
    "print(\"\\nBasic ID validation done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature engineering done.\n",
      "User columns now include: ['user_id', 'segment', 'name', 'email', 'username', 'birthdate', 'gender', 'wave_fraud_boost', 'device_id', 'phone', 'country_code', 'created_at', 'burst_signup', 'fraud_label', 'is_ring_leader', 'email_domain', 'ip_count', 'num_fraud_biz_owned', 'segment_code', 'ip_count_log', 'country_watch', 'phone_susp', 'email_susp']\n",
      "Business columns now include: ['business_id', 'business_name', 'registration_country', 'incorporation_date', 'owner_name', 'fraud_label', 'watchlist_regctry', 'susp_name_flag', 'biz_age_days', 'biz_age_log']\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# Cell 5: Feature Engineering\n",
    "# =====================================================================\n",
    "def encode_segment(seg):\n",
    "    mapping = {\"casual\": 0, \"smb_owner\": 1, \"enterprise\": 2, \"money_mule\": 3}\n",
    "    return mapping.get(seg, 0)\n",
    "\n",
    "def watchlist_country(ctry):\n",
    "    if isinstance(ctry, str) and ctry.upper() in [\"NK\",\"IR\",\"SY\",\"CU\",\"AF\",\"SO\",\"LY\"]:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def suspicious_name(bname):\n",
    "    suspicious_keywords = [\"test\", \"fake\", \"shell\", \"phantom\", \"bogus\", \"shady\"]\n",
    "    bname_lower = str(bname).lower()\n",
    "    return 1 if any(kw in bname_lower for kw in suspicious_keywords) else 0\n",
    "\n",
    "# -- Feature eng for df_users\n",
    "df_users['segment_code'] = df_users['segment'].apply(encode_segment).fillna(0).astype(int)\n",
    "df_users['burst_signup'] = df_users.get('burst_signup', False).astype(int)\n",
    "df_users['is_ring_leader'] = df_users.get('is_ring_leader', False).astype(int)\n",
    "\n",
    "# For log transform, handle negative or missing ip_count\n",
    "df_users['ip_count'] = df_users['ip_count'].fillna(0)\n",
    "df_users['ip_count'] = df_users['ip_count'].clip(lower=0)\n",
    "df_users['ip_count_log'] = np.log1p(df_users['ip_count'])\n",
    "\n",
    "df_users['country_watch'] = df_users['country_code'].fillna('').apply(watchlist_country)\n",
    "\n",
    "def phone_suspicious(phone):\n",
    "    phone = str(phone)\n",
    "    if len(phone) < 7:\n",
    "        return 1\n",
    "    if '+999' in phone or '666-666' in phone:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def email_suspicious(email):\n",
    "    email = str(email).lower()\n",
    "    suspicious_doms = [\"@tempmail.xyz\", \"@fakemail.com\", \"@guerrillamail.com\"]\n",
    "    return 1 if any(dom in email for dom in suspicious_doms) else 0\n",
    "\n",
    "df_users['phone_susp'] = df_users['phone'].fillna('').apply(phone_suspicious)\n",
    "df_users['email_susp'] = df_users['email'].fillna('').apply(email_suspicious)\n",
    "\n",
    "# -- Feature eng for df_businesses\n",
    "df_businesses['watchlist_regctry'] = df_businesses['registration_country'].fillna('').apply(watchlist_country)\n",
    "df_businesses['susp_name_flag'] = df_businesses['business_name'].apply(suspicious_name)\n",
    "\n",
    "def days_since_incorp(date):\n",
    "    if pd.isnull(date):\n",
    "        return 0\n",
    "    ref_date = pd.Timestamp.now()\n",
    "    delta = ref_date - pd.to_datetime(date)\n",
    "    return max(delta.days, 0)\n",
    "\n",
    "df_businesses['biz_age_days'] = df_businesses['incorporation_date'].apply(days_since_incorp)\n",
    "df_businesses['biz_age_log']  = np.log1p(df_businesses['biz_age_days'].fillna(0))\n",
    "\n",
    "print(\"\\nFeature engineering done.\")\n",
    "print(\"User columns now include:\", df_users.columns.tolist())\n",
    "print(\"Business columns now include:\", df_businesses.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Constructed 10274 user-user edges (incl duplicates).\n",
      "Constructed 221613 user-business edges.\n",
      "Constructed 100000 user-ip edges.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# Cell 6: Node ID Assignment & Edge Building (3 node types: user, biz, ip)\n",
    "# =====================================================================\n",
    "num_users = len(df_users)\n",
    "num_biz   = len(df_businesses)\n",
    "num_ips   = len(df_ip_nodes)\n",
    "\n",
    "# user_id => node_id\n",
    "df_users['node_id'] = df_users['user_id'] - 1\n",
    "df_businesses['node_id'] = df_businesses['business_id'] - 1\n",
    "df_ip_nodes['node_id'] = df_ip_nodes['ip_id'] - 1  # if we want IP node indices\n",
    "\n",
    "# -- Build user->user edges\n",
    "df_user_user['from_id_0'] = df_user_user['from_user_id'] - 1\n",
    "df_user_user['to_id_0']   = df_user_user['to_user_id']   - 1\n",
    "\n",
    "edges_user_user = []\n",
    "for _, row in df_user_user.iterrows():\n",
    "    f_id = row['from_id_0']\n",
    "    t_id = row['to_id_0']\n",
    "    edges_user_user.append((f_id, t_id))\n",
    "    edges_user_user.append((t_id, f_id))  # undirected\n",
    "\n",
    "# -- Build user->biz edges\n",
    "df_user_biz['user_id_0'] = df_user_biz['user_id'] - 1\n",
    "df_user_biz['biz_id_0']  = df_user_biz['business_id'] - 1\n",
    "\n",
    "edges_user_biz = []\n",
    "for _, row in df_user_biz.iterrows():\n",
    "    edges_user_biz.append((row['user_id_0'], row['biz_id_0']))\n",
    "\n",
    "# -- Build user->ip edges\n",
    "df_user_ip['user_id_0'] = df_user_ip['user_id'] - 1\n",
    "df_user_ip['ip_id_0']   = df_user_ip['ip_id']   - 1\n",
    "\n",
    "edges_user_ip = []\n",
    "for _, row in df_user_ip.iterrows():\n",
    "    edges_user_ip.append((row['user_id_0'], row['ip_id_0']))\n",
    "\n",
    "print(f\"\\nConstructed {len(edges_user_user)} user-user edges (incl duplicates).\")\n",
    "print(f\"Constructed {len(edges_user_biz)} user-business edges.\")\n",
    "print(f\"Constructed {len(edges_user_ip)} user-ip edges.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User feature shape: (100000, 7), Business feature shape: (10000, 3), IP feature shape: (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# Cell 7: Creating Feature Arrays & Labels\n",
    "# =====================================================================\n",
    "# For user nodes\n",
    "user_feature_cols = [\n",
    "    'segment_code','is_ring_leader','ip_count_log','phone_susp',\n",
    "    'email_susp','country_watch','burst_signup'\n",
    "]\n",
    "df_users_sorted = df_users.sort_values('node_id')\n",
    "user_features = df_users_sorted[user_feature_cols].to_numpy(dtype=np.float32)\n",
    "user_labels   = df_users_sorted['fraud_label'].fillna(0).astype(int).to_numpy()\n",
    "\n",
    "# For business nodes\n",
    "biz_feature_cols = [\n",
    "    'watchlist_regctry','susp_name_flag','biz_age_log'\n",
    "]\n",
    "df_biz_sorted = df_businesses.sort_values('node_id')\n",
    "biz_features  = df_biz_sorted[biz_feature_cols].to_numpy(dtype=np.float32)\n",
    "biz_labels    = df_biz_sorted['fraud_label'].fillna(0).astype(int).to_numpy()\n",
    "\n",
    "# (Optional) For IP nodes: if you want to do multi-node-type GNN\n",
    "# We currently have no direct \"fraud_label\" for IP, but you can create one if desired.\n",
    "df_ip_sorted = df_ip_nodes.sort_values('node_id')\n",
    "\n",
    "# If you want IP features, here's a placeholder approach:\n",
    "# e.g., no advanced feature, just store a \"0\" or \"1\" if IP is in a suspicious range\n",
    "df_ip_sorted['susp_ip_flag'] = 0  # or some logic\n",
    "ip_feature_cols = ['susp_ip_flag']\n",
    "ip_features = df_ip_sorted[ip_feature_cols].to_numpy(dtype=np.float32)\n",
    "\n",
    "# If you want IP labels (not typical unless you have a reason):\n",
    "ip_labels = np.zeros(len(df_ip_sorted), dtype=int)\n",
    "\n",
    "print(f\"User feature shape: {user_features.shape}, Business feature shape: {biz_features.shape}, IP feature shape: {ip_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User node splits => train=70000, val=15000, test=15000\n",
      "Business node splits => train=7000, val=1500, test=1500\n",
      "IP node splits => train=3500, val=750, test=750\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# Cell 8: Train/Val/Test Splits\n",
    "# =====================================================================\n",
    "train_mask_users = None\n",
    "val_mask_users   = None\n",
    "test_mask_users  = None\n",
    "\n",
    "train_mask_biz = None\n",
    "val_mask_biz   = None\n",
    "test_mask_biz  = None\n",
    "\n",
    "# If you want splitting for IP, define them here\n",
    "train_mask_ip = None\n",
    "val_mask_ip   = None\n",
    "test_mask_ip  = None\n",
    "\n",
    "if DO_SPLIT:\n",
    "    # -- Users\n",
    "    user_node_ids = df_users_sorted['node_id'].to_numpy()\n",
    "    np.random.shuffle(user_node_ids)\n",
    "\n",
    "    n_train_users = int(TRAIN_RATIO * num_users)\n",
    "    n_val_users   = int(VAL_RATIO * num_users)\n",
    "\n",
    "    train_ids_user = user_node_ids[:n_train_users]\n",
    "    val_ids_user   = user_node_ids[n_train_users : n_train_users + n_val_users]\n",
    "    test_ids_user  = user_node_ids[n_train_users + n_val_users : ]\n",
    "\n",
    "    train_mask_users = np.zeros(num_users, dtype=bool)\n",
    "    val_mask_users   = np.zeros(num_users, dtype=bool)\n",
    "    test_mask_users  = np.zeros(num_users, dtype=bool)\n",
    "\n",
    "    train_mask_users[train_ids_user] = True\n",
    "    val_mask_users[val_ids_user]     = True\n",
    "    test_mask_users[test_ids_user]   = True\n",
    "\n",
    "    print(f\"\\nUser node splits => train={train_mask_users.sum()}, val={val_mask_users.sum()}, test={test_mask_users.sum()}\")\n",
    "\n",
    "    # -- Businesses (multi-task only if SINGLE_TASK_USER_ONLY=False)\n",
    "    if not SINGLE_TASK_USER_ONLY:\n",
    "        biz_node_ids = df_biz_sorted['node_id'].to_numpy()\n",
    "        np.random.shuffle(biz_node_ids)\n",
    "\n",
    "        n_train_biz = int(TRAIN_RATIO * num_biz)\n",
    "        n_val_biz   = int(VAL_RATIO * num_biz)\n",
    "\n",
    "        train_ids_biz = biz_node_ids[:n_train_biz]\n",
    "        val_ids_biz   = biz_node_ids[n_train_biz : n_train_biz + n_val_biz]\n",
    "        test_ids_biz  = biz_node_ids[n_train_biz + n_val_biz : ]\n",
    "\n",
    "        train_mask_biz = np.zeros(num_biz, dtype=bool)\n",
    "        val_mask_biz   = np.zeros(num_biz, dtype=bool)\n",
    "        test_mask_biz  = np.zeros(num_biz, dtype=bool)\n",
    "\n",
    "        train_mask_biz[train_ids_biz] = True\n",
    "        val_mask_biz[val_ids_biz]     = True\n",
    "        test_mask_biz[test_ids_biz]   = True\n",
    "\n",
    "        print(f\"Business node splits => train={train_mask_biz.sum()}, val={val_mask_biz.sum()}, test={test_mask_biz.sum()}\")\n",
    "\n",
    "    # -- IP (Optional)\n",
    "    # If you want to classify IPs as suspicious or not, define a label. Then do a split:\n",
    "    # e.g.:\n",
    "    ip_node_ids = df_ip_sorted['node_id'].to_numpy()\n",
    "    np.random.shuffle(ip_node_ids)\n",
    "    n_train_ips = int(TRAIN_RATIO * num_ips)\n",
    "    n_val_ips   = int(VAL_RATIO * num_ips)\n",
    "\n",
    "    train_ids_ip = ip_node_ids[:n_train_ips]\n",
    "    val_ids_ip   = ip_node_ids[n_train_ips : n_train_ips + n_val_ips]\n",
    "    test_ids_ip  = ip_node_ids[n_train_ips + n_val_ips : ]\n",
    "\n",
    "    train_mask_ip = np.zeros(num_ips, dtype=bool)\n",
    "    val_mask_ip   = np.zeros(num_ips, dtype=bool)\n",
    "    test_mask_ip  = np.zeros(num_ips, dtype=bool)\n",
    "\n",
    "    train_mask_ip[train_ids_ip] = True\n",
    "    val_mask_ip[val_ids_ip]     = True\n",
    "    test_mask_ip[test_ids_ip]   = True\n",
    "\n",
    "    print(f\"IP node splits => train={train_mask_ip.sum()}, val={val_mask_ip.sum()}, test={test_mask_ip.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All processed data saved to: /Users/harshil/Development/GitHub_Repos/VeriShield-AI-Financial-Verification-Platform/verishield_ml_experiments/data_generators/data/medium_fraud/processed_gnn\n",
      "Data prep complete (with IP nodes & user->ip relationships)!\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# Cell 9: Saving Processed Data\n",
    "# =====================================================================\n",
    "# 1) Node features + labels\n",
    "np.save(os.path.join(processed_dir, \"user_features.npy\"), user_features)\n",
    "np.save(os.path.join(processed_dir, \"user_labels.npy\"),   user_labels)\n",
    "np.save(os.path.join(processed_dir, \"biz_features.npy\"),  biz_features)\n",
    "np.save(os.path.join(processed_dir, \"biz_labels.npy\"),    biz_labels)\n",
    "# IP features + labels (if you're doing multi-node-type GNN):\n",
    "np.save(os.path.join(processed_dir, \"ip_features.npy\"),  ip_features)\n",
    "np.save(os.path.join(processed_dir, \"ip_labels.npy\"),    ip_labels)  # zero by default\n",
    "\n",
    "# 2) Edges\n",
    "user_user_arr = np.array(edges_user_user, dtype=np.int64).T  # shape (2, E_uu)\n",
    "user_biz_arr  = np.array(edges_user_biz,  dtype=np.int64).T  # shape (2, E_ub)\n",
    "user_ip_arr   = np.array(edges_user_ip,  dtype=np.int64).T   # shape (2, E_ui)\n",
    "\n",
    "np.save(os.path.join(processed_dir, \"edge_user_user.npy\"), user_user_arr)\n",
    "np.save(os.path.join(processed_dir, \"edge_user_biz.npy\"),  user_biz_arr)\n",
    "np.save(os.path.join(processed_dir, \"edge_user_ip.npy\"),   user_ip_arr)\n",
    "\n",
    "# 3) Masks\n",
    "if DO_SPLIT:\n",
    "    # user masks\n",
    "    np.save(os.path.join(processed_dir, \"train_mask_users.npy\"), train_mask_users)\n",
    "    np.save(os.path.join(processed_dir, \"val_mask_users.npy\"),   val_mask_users)\n",
    "    np.save(os.path.join(processed_dir, \"test_mask_users.npy\"),  test_mask_users)\n",
    "\n",
    "    # business masks\n",
    "    if not SINGLE_TASK_USER_ONLY and train_mask_biz is not None:\n",
    "        np.save(os.path.join(processed_dir, \"train_mask_biz.npy\"), train_mask_biz)\n",
    "        np.save(os.path.join(processed_dir, \"val_mask_biz.npy\"),   val_mask_biz)\n",
    "        np.save(os.path.join(processed_dir, \"test_mask_biz.npy\"),  test_mask_biz)\n",
    "\n",
    "    # ip masks\n",
    "    # only relevant if you want to do IP classification\n",
    "    if train_mask_ip is not None:\n",
    "        np.save(os.path.join(processed_dir, \"train_mask_ip.npy\"), train_mask_ip)\n",
    "        np.save(os.path.join(processed_dir, \"val_mask_ip.npy\"),   val_mask_ip)\n",
    "        np.save(os.path.join(processed_dir, \"test_mask_ip.npy\"),  test_mask_ip)\n",
    "\n",
    "# 4) Metadata JSON\n",
    "metadata = {\n",
    "    \"scenario\": SCENARIO,\n",
    "    \"num_users\": num_users,\n",
    "    \"num_businesses\": num_biz,\n",
    "    \"num_ips\": num_ips,\n",
    "    \"user_feature_cols\": user_feature_cols,\n",
    "    \"biz_feature_cols\": biz_feature_cols,\n",
    "    \"ip_feature_cols\": ip_feature_cols,  # if you want to track them\n",
    "    \"do_split\": DO_SPLIT,\n",
    "    \"train_ratio\": TRAIN_RATIO,\n",
    "    \"val_ratio\": VAL_RATIO,\n",
    "    \"test_ratio\": TEST_RATIO,\n",
    "    \"SINGLE_TASK_USER_ONLY\": SINGLE_TASK_USER_ONLY,\n",
    "    \"edges_user_user_count\": user_user_arr.shape[1],\n",
    "    \"edges_user_biz_count\":  user_biz_arr.shape[1],\n",
    "    \"edges_user_ip_count\":   user_ip_arr.shape[1],\n",
    "}\n",
    "\n",
    "with open(os.path.join(processed_dir, \"metadata.json\"), \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"\\nAll processed data saved to:\", processed_dir)\n",
    "print(\"Data prep complete (with IP nodes & user->ip relationships)!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
