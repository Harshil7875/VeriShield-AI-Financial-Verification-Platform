{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Notebook: 02-Model-Training-GNN-PyTorch-4.ipynb (Extended for IP & Multi-Task)\n",
    "# =============================================================================\n",
    "# This notebook demonstrates multi-task GNN training on processed GNN data\n",
    "# that includes user, business, and IP nodes, plus masks for train/val/test.\n",
    "# Now using synergy-based IP labels from your new data pipeline,\n",
    "# and a correct steps_per_epoch formula for mini-batching (if desired).\n",
    "\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import HeteroConv, SAGEConv, Linear\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed GNN data at: /Users/harshil/Developer/GitHub_Repos/VeriShield-AI-Financial-Verification-Platform/verishield_ml_experiments/data_generators/data-v1/high_fraud/processed_gnn\n",
      "MULTI_TASK=True, IP_CLASSIFICATION=True\n",
      "HIDDEN_DIM=64, LR=0.001, EPOCHS=200, BATCH_SIZE=1024\n",
      "Initial STEPS_PER_EPOCH=1 (may override after loading data)\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Cell 2: Configuration\n",
    "# --------------------------------------------------------------------------------\n",
    "# Path to synergy-based data (already preprocessed into .npy files)\n",
    "PROCESSED_DIR = (\n",
    "    \"/Users/harshil/Developer/\"\n",
    "    \"GitHub_Repos/VeriShield-AI-Financial-Verification-Platform/\"\n",
    "    \"verishield_ml_experiments/data_generators/data-v1/high_fraud/processed_gnn\"\n",
    ")\n",
    "\n",
    "# Multi-task toggles\n",
    "MULTI_TASK = True          # or False if only user classification\n",
    "IP_CLASSIFICATION = True   # classify IP nodes?\n",
    "\n",
    "# GNN Hyperparams\n",
    "HIDDEN_DIM = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 200\n",
    "\n",
    "# BATCH-SIZE & STEPS_PER_EPOCH:\n",
    "#  - If you do mini-batch training, steps_per_epoch = ceil(train_size / BATCH_SIZE).\n",
    "#  - If you do full-batch (the code currently does a single forward on the entire graph),\n",
    "#    then steps_per_epoch=1 is typical. \n",
    "BATCH_SIZE = 1024   # example batch size if you do mini-batching\n",
    "STEPS_PER_EPOCH = 1 # we'll override this later once we load train masks, if we want\n",
    "\n",
    "PRINT_EVERY = 1\n",
    "\n",
    "print(f\"Processed GNN data at: {PROCESSED_DIR}\")\n",
    "print(f\"MULTI_TASK={MULTI_TASK}, IP_CLASSIFICATION={IP_CLASSIFICATION}\")\n",
    "print(f\"HIDDEN_DIM={HIDDEN_DIM}, LR={LEARNING_RATE}, EPOCHS={EPOCHS}, BATCH_SIZE={BATCH_SIZE}\")\n",
    "print(f\"Initial STEPS_PER_EPOCH={STEPS_PER_EPOCH} (may override after loading data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded user_features.npy, shape=(50000, 7), dtype=float32\n",
      "Loaded user_labels.npy, shape=(50000,), dtype=int64\n",
      "Loaded biz_features.npy, shape=(10000, 3), dtype=float32\n",
      "Loaded biz_labels.npy, shape=(10000,), dtype=int64\n",
      "Loaded ip_features.npy, shape=(8000, 0), dtype=float32\n",
      "Loaded ip_labels.npy, shape=(8000,), dtype=int64\n",
      "Loaded edge_user_user.npy, shape=(2, 4918), dtype=int64\n",
      "Loaded edge_user_biz.npy, shape=(2, 110731), dtype=int64\n",
      "Loaded edge_user_ip.npy, shape=(2, 50000), dtype=int64\n",
      "Loaded train_mask_users.npy, shape=(50000,), dtype=bool\n",
      "Loaded val_mask_users.npy, shape=(50000,), dtype=bool\n",
      "Loaded test_mask_users.npy, shape=(50000,), dtype=bool\n",
      "Loaded train_mask_biz.npy, shape=(10000,), dtype=bool\n",
      "Loaded val_mask_biz.npy, shape=(10000,), dtype=bool\n",
      "Loaded test_mask_biz.npy, shape=(10000,), dtype=bool\n",
      "Loaded train_mask_ip.npy, shape=(8000,), dtype=bool\n",
      "Loaded val_mask_ip.npy, shape=(8000,), dtype=bool\n",
      "Loaded test_mask_ip.npy, shape=(8000,), dtype=bool\n",
      "\n",
      "metadata.json contents:\n",
      "  scenario: high_fraud\n",
      "  num_users: 50000\n",
      "  num_businesses: 10000\n",
      "  num_ips: 8000\n",
      "  user_feature_cols: ['segment_code', 'is_ring_leader', 'ip_count_log', 'phone_susp', 'email_susp', 'country_watch', 'burst_signup']\n",
      "  biz_feature_cols: ['watchlist_regctry', 'susp_name_flag', 'biz_age_log']\n",
      "  ip_feature_cols: []\n",
      "  do_split: True\n",
      "  train_ratio: 0.7\n",
      "  val_ratio: 0.15\n",
      "  test_ratio: 0.15\n",
      "  SINGLE_TASK_USER_ONLY: False\n",
      "  edges_user_user_count: 4918\n",
      "  edges_user_biz_count: 110731\n",
      "  edges_user_ip_count: 50000\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Cell 3: Loading .npy Arrays\n",
    "# --------------------------------------------------------------------------------\n",
    "def load_npy(filename):\n",
    "    path = os.path.join(PROCESSED_DIR, filename)\n",
    "    arr = np.load(path)\n",
    "    print(f\"Loaded {filename}, shape={arr.shape}, dtype={arr.dtype}\")\n",
    "    return arr\n",
    "\n",
    "user_features  = torch.from_numpy(load_npy(\"user_features.npy\")).float()\n",
    "user_labels    = torch.from_numpy(load_npy(\"user_labels.npy\")).long()\n",
    "biz_features   = torch.from_numpy(load_npy(\"biz_features.npy\")).float()\n",
    "biz_labels     = torch.from_numpy(load_npy(\"biz_labels.npy\")).long()\n",
    "ip_features    = torch.from_numpy(load_npy(\"ip_features.npy\")).float()\n",
    "ip_labels      = torch.from_numpy(load_npy(\"ip_labels.npy\")).long()\n",
    "\n",
    "edge_user_user = torch.from_numpy(load_npy(\"edge_user_user.npy\")).long()\n",
    "edge_user_biz  = torch.from_numpy(load_npy(\"edge_user_biz.npy\")).long()\n",
    "edge_user_ip   = torch.from_numpy(load_npy(\"edge_user_ip.npy\")).long()\n",
    "\n",
    "# Load train/val/test masks\n",
    "train_mask_users = None\n",
    "val_mask_users   = None\n",
    "test_mask_users  = None\n",
    "try:\n",
    "    train_mask_users = torch.from_numpy(load_npy(\"train_mask_users.npy\")).bool()\n",
    "    val_mask_users   = torch.from_numpy(load_npy(\"val_mask_users.npy\")).bool()\n",
    "    test_mask_users  = torch.from_numpy(load_npy(\"test_mask_users.npy\")).bool()\n",
    "except FileNotFoundError:\n",
    "    print(\"No user masks found; will do full-batch on users if MULTI_TASK=False.\")\n",
    "\n",
    "train_mask_biz = None\n",
    "val_mask_biz   = None\n",
    "test_mask_biz  = None\n",
    "if MULTI_TASK:\n",
    "    try:\n",
    "        train_mask_biz = torch.from_numpy(load_npy(\"train_mask_biz.npy\")).bool()\n",
    "        val_mask_biz   = torch.from_numpy(load_npy(\"val_mask_biz.npy\")).bool()\n",
    "        test_mask_biz  = torch.from_numpy(load_npy(\"test_mask_biz.npy\")).bool()\n",
    "    except FileNotFoundError:\n",
    "        print(\"No business masks found; can't do multi-task effectively without them.\")\n",
    "\n",
    "train_mask_ip = None\n",
    "val_mask_ip   = None\n",
    "test_mask_ip  = None\n",
    "if IP_CLASSIFICATION:\n",
    "    try:\n",
    "        train_mask_ip = torch.from_numpy(load_npy(\"train_mask_ip.npy\")).bool()\n",
    "        val_mask_ip   = torch.from_numpy(load_npy(\"val_mask_ip.npy\")).bool()\n",
    "        test_mask_ip  = torch.from_numpy(load_npy(\"test_mask_ip.npy\")).bool()\n",
    "    except FileNotFoundError:\n",
    "        print(\"No IP masks found; IP classification won't be possible without them.\")\n",
    "\n",
    "# Optionally load metadata.json\n",
    "meta_path = os.path.join(PROCESSED_DIR, \"metadata.json\")\n",
    "if os.path.isfile(meta_path):\n",
    "    with open(meta_path, \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "    print(\"\\nmetadata.json contents:\")\n",
    "    for k,v in metadata.items():\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted STEPS_PER_EPOCH = 35 (from user train count=35000, batch_size=1024)\n",
      "Final STEPS_PER_EPOCH: 35\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Cell 4: Adjust STEPS_PER_EPOCH if Doing Mini-Batch\n",
    "# --------------------------------------------------------------------------------\n",
    "# The code below still does \"full-batch\" training, but here's how you'd compute steps_per_epoch \n",
    "# if you truly do mini-batching at the node level (not shown in detail here).\n",
    "# We'll pick 'user' training set size to define steps_per_epoch, for instance:\n",
    "\n",
    "if train_mask_users is not None:\n",
    "    train_user_count = train_mask_users.sum().item()\n",
    "    if train_user_count > 0:\n",
    "        STEPS_PER_EPOCH = math.ceil(train_user_count / BATCH_SIZE)\n",
    "        print(f\"Adjusted STEPS_PER_EPOCH = {STEPS_PER_EPOCH} (from user train count={train_user_count}, batch_size={BATCH_SIZE})\")\n",
    "    else:\n",
    "        # fallback to 1 if no users in train set\n",
    "        STEPS_PER_EPOCH = 1\n",
    "else:\n",
    "    # full-batch\n",
    "    STEPS_PER_EPOCH = 1\n",
    "\n",
    "print(f\"Final STEPS_PER_EPOCH: {STEPS_PER_EPOCH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HeteroData object created with node_types: ['user', 'business', 'ip']\n",
      "Edge types: [('user', 'user_user', 'user'), ('user', 'user_business', 'business'), ('user', 'user_ip', 'ip'), ('business', 'rev_user_business', 'user'), ('ip', 'rev_user_ip', 'user')]\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Cell 5: Build a HeteroData Graph (Users, Businesses, IPs)\n",
    "# --------------------------------------------------------------------------------\n",
    "data = HeteroData()\n",
    "\n",
    "# user nodes\n",
    "data['user'].x = user_features\n",
    "data['user'].y = user_labels\n",
    "if train_mask_users is not None:\n",
    "    data['user'].train_mask = train_mask_users\n",
    "    data['user'].val_mask   = val_mask_users\n",
    "    data['user'].test_mask  = test_mask_users\n",
    "\n",
    "# business nodes\n",
    "data['business'].x = biz_features\n",
    "data['business'].y = biz_labels\n",
    "if MULTI_TASK and train_mask_biz is not None:\n",
    "    data['business'].train_mask = train_mask_biz\n",
    "    data['business'].val_mask   = val_mask_biz\n",
    "    data['business'].test_mask  = test_mask_biz\n",
    "\n",
    "# ip nodes\n",
    "data['ip'].x = ip_features\n",
    "data['ip'].y = ip_labels\n",
    "if IP_CLASSIFICATION and train_mask_ip is not None:\n",
    "    data['ip'].train_mask  = train_mask_ip\n",
    "    data['ip'].val_mask    = val_mask_ip\n",
    "    data['ip'].test_mask   = test_mask_ip\n",
    "\n",
    "# Edges\n",
    "data[('user','user_user','user')].edge_index = edge_user_user\n",
    "data[('user','user_business','business')].edge_index = edge_user_biz\n",
    "data[('user','user_ip','ip')].edge_index = edge_user_ip\n",
    "\n",
    "# Reverse edges for message passing in both directions\n",
    "rev_user_biz = torch.flip(edge_user_biz, dims=[0])\n",
    "data[('business','rev_user_business','user')].edge_index = rev_user_biz\n",
    "\n",
    "rev_user_ip = torch.flip(edge_user_ip, dims=[0])\n",
    "data[('ip','rev_user_ip','user')].edge_index = rev_user_ip\n",
    "\n",
    "print(\"\\nHeteroData object created with node_types:\", list(data.node_types))\n",
    "print(\"Edge types:\", list(data.edge_types))\n",
    "\n",
    "# Move entire data to GPU\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FraudGNN(\n",
      "  (conv1): HeteroConv(num_relations=5)\n",
      "  (conv2): HeteroConv(num_relations=5)\n",
      "  (user_lin): Linear(64, 2, bias=True)\n",
      "  (business_lin): Linear(64, 2, bias=True)\n",
      "  (ip_lin): Linear(64, 2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Cell 6: Define a GNN Model (Supports Multi-Edges & Optional Multi-Task)\n",
    "# --------------------------------------------------------------------------------\n",
    "class FraudGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim=64, user_out=2, biz_out=2, ip_out=2,\n",
    "                 multi_task=False, ip_task=False):\n",
    "        \"\"\"\n",
    "        multi_task=True => We'll produce heads for user + business\n",
    "        ip_task=True    => We'll also produce a head for IP\n",
    "\n",
    "        Otherwise, we do a single user head only.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.multi_task = multi_task\n",
    "        self.ip_task = ip_task\n",
    "\n",
    "        # Single HeteroConv for all relevant edges:\n",
    "        self.conv1 = HeteroConv({\n",
    "            ('user','user_user','user'): SAGEConv((-1, -1), hidden_dim),\n",
    "            ('user','user_business','business'): SAGEConv((-1, -1), hidden_dim),\n",
    "            ('business','rev_user_business','user'): SAGEConv((-1, -1), hidden_dim),\n",
    "            ('user','user_ip','ip'): SAGEConv((-1, -1), hidden_dim),\n",
    "            ('ip','rev_user_ip','user'): SAGEConv((-1, -1), hidden_dim),\n",
    "        }, aggr='mean')\n",
    "\n",
    "        self.conv2 = HeteroConv({\n",
    "            ('user','user_user','user'): SAGEConv((-1, -1), hidden_dim),\n",
    "            ('user','user_business','business'): SAGEConv((-1, -1), hidden_dim),\n",
    "            ('business','rev_user_business','user'): SAGEConv((-1, -1), hidden_dim),\n",
    "            ('user','user_ip','ip'): SAGEConv((-1, -1), hidden_dim),\n",
    "            ('ip','rev_user_ip','user'): SAGEConv((-1, -1), hidden_dim),\n",
    "        }, aggr='mean')\n",
    "\n",
    "        # Final linear heads\n",
    "        self.user_lin = Linear(hidden_dim, user_out)\n",
    "\n",
    "        if multi_task:\n",
    "            self.business_lin = Linear(hidden_dim, biz_out)\n",
    "        else:\n",
    "            self.business_lin = None\n",
    "\n",
    "        if ip_task:\n",
    "            self.ip_lin = Linear(hidden_dim, ip_out)\n",
    "        else:\n",
    "            self.ip_lin = None\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        # 1st layer\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        for ntype, x in x_dict.items():\n",
    "            x_dict[ntype] = F.relu(x)\n",
    "\n",
    "        # 2nd layer\n",
    "        x_dict = self.conv2(x_dict, edge_index_dict)\n",
    "        for ntype, x in x_dict.items():\n",
    "            x_dict[ntype] = F.relu(x)\n",
    "\n",
    "        return x_dict\n",
    "\n",
    "    def forward_user(self, emb_dict):\n",
    "        return self.user_lin(emb_dict['user'])\n",
    "\n",
    "    def forward_business(self, emb_dict):\n",
    "        if self.business_lin is None:\n",
    "            return None\n",
    "        return self.business_lin(emb_dict['business'])\n",
    "\n",
    "    def forward_ip(self, emb_dict):\n",
    "        if self.ip_lin is None:\n",
    "            return None\n",
    "        return self.ip_lin(emb_dict['ip'])\n",
    "\n",
    "model = FraudGNN(\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    user_out=2,         # user classification: legit/fraud\n",
    "    biz_out=2,          # business classification\n",
    "    ip_out=2,           # IP classification\n",
    "    multi_task=MULTI_TASK,\n",
    "    ip_task=IP_CLASSIFICATION\n",
    ").to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Cell 7: Training Setup (Multi-Task or Single)\n",
    "# --------------------------------------------------------------------------------\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def train_step():\n",
    "    \"\"\"\n",
    "    One train step on the entire graph (masked to train sets).\n",
    "    If truly doing mini-batch, you'd sample subgraph or node-batches here\n",
    "    instead of passing the entire data each step.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    emb_dict = model(data.x_dict, data.edge_index_dict)\n",
    "\n",
    "    # User classification\n",
    "    user_logits = model.forward_user(emb_dict)\n",
    "    train_mask_u = data['user'].train_mask if 'train_mask' in data['user'] else None\n",
    "    user_loss = 0.\n",
    "    if train_mask_u is not None:\n",
    "        masked_logits = user_logits[train_mask_u]\n",
    "        masked_labels = data['user'].y[train_mask_u]\n",
    "        user_loss = F.cross_entropy(masked_logits, masked_labels)\n",
    "    else:\n",
    "        user_loss = F.cross_entropy(user_logits, data['user'].y)\n",
    "\n",
    "    # Business classification (multi-task)\n",
    "    biz_loss = 0.\n",
    "    if model.multi_task and 'train_mask' in data['business']:\n",
    "        biz_logits = model.forward_business(emb_dict)\n",
    "        train_mask_b = data['business'].train_mask\n",
    "        masked_biz_logits = biz_logits[train_mask_b]\n",
    "        masked_biz_labels = data['business'].y[train_mask_b]\n",
    "        biz_loss = F.cross_entropy(masked_biz_logits, masked_biz_labels)\n",
    "\n",
    "    # IP classification (ip_task)\n",
    "    ip_loss = 0.\n",
    "    if model.ip_task and 'train_mask' in data['ip']:\n",
    "        ip_logits = model.forward_ip(emb_dict)\n",
    "        train_mask_i = data['ip'].train_mask\n",
    "        masked_ip_logits = ip_logits[train_mask_i]\n",
    "        masked_ip_labels = data['ip'].y[train_mask_i]\n",
    "        ip_loss = F.cross_entropy(masked_ip_logits, masked_ip_labels)\n",
    "\n",
    "    total_loss = user_loss + biz_loss + ip_loss\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(total_loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_user(mask_name='val_mask'):\n",
    "    \"\"\"Compute user accuracy on val/test mask.\"\"\"\n",
    "    model.eval()\n",
    "    emb_dict = model(data.x_dict, data.edge_index_dict)\n",
    "    user_logits = model.forward_user(emb_dict)\n",
    "    if mask_name not in data['user']:\n",
    "        preds = user_logits.argmax(dim=-1)\n",
    "        correct = (preds == data['user'].y).sum()\n",
    "        return float(correct / len(data['user'].y))\n",
    "    mask = data['user'][mask_name]\n",
    "    logits_masked = user_logits[mask]\n",
    "    labels_masked = data['user'].y[mask]\n",
    "    preds = logits_masked.argmax(dim=-1)\n",
    "    correct = (preds == labels_masked).sum()\n",
    "    return float(correct / mask.sum())\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_biz(mask_name='val_mask'):\n",
    "    \"\"\"Compute business accuracy if multi-task is enabled.\"\"\"\n",
    "    if not model.multi_task or mask_name not in data['business']:\n",
    "        return 0.0\n",
    "    model.eval()\n",
    "    emb_dict = model(data.x_dict, data.edge_index_dict)\n",
    "    biz_logits = model.forward_business(emb_dict)\n",
    "    mask = data['business'][mask_name]\n",
    "    logits_masked = biz_logits[mask]\n",
    "    labels_masked = data['business'].y[mask]\n",
    "    preds = logits_masked.argmax(dim=-1)\n",
    "    correct = (preds == labels_masked).sum()\n",
    "    return float(correct / mask.sum())\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_ip(mask_name='val_mask'):\n",
    "    \"\"\"Compute IP accuracy if IP classification is enabled.\"\"\"\n",
    "    if not model.ip_task or mask_name not in data['ip']:\n",
    "        return 0.0\n",
    "    model.eval()\n",
    "    emb_dict = model(data.x_dict, data.edge_index_dict)\n",
    "    ip_logits = model.forward_ip(emb_dict)\n",
    "    mask = data['ip'][mask_name]\n",
    "    logits_masked = ip_logits[mask]\n",
    "    labels_masked = data['ip'].y[mask]\n",
    "    preds = logits_masked.argmax(dim=-1)\n",
    "    correct = (preds == labels_masked).sum()\n",
    "    return float(correct / mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 => loss=1.6061, user_val_acc=0.6801, biz_val_acc=0.9227, ip_val_acc=0.6417\n",
      "Epoch 2/200 => loss=1.4833, user_val_acc=0.6801, biz_val_acc=0.9227, ip_val_acc=0.6500\n",
      "Epoch 3/200 => loss=1.4694, user_val_acc=0.6867, biz_val_acc=0.9227, ip_val_acc=0.6533\n",
      "Epoch 4/200 => loss=1.4574, user_val_acc=0.6951, biz_val_acc=0.9227, ip_val_acc=0.6492\n",
      "Epoch 5/200 => loss=1.4468, user_val_acc=0.7029, biz_val_acc=0.9227, ip_val_acc=0.6492\n",
      "Epoch 6/200 => loss=1.4379, user_val_acc=0.7057, biz_val_acc=0.9227, ip_val_acc=0.6467\n",
      "Epoch 7/200 => loss=1.4309, user_val_acc=0.7076, biz_val_acc=0.9227, ip_val_acc=0.6408\n",
      "Epoch 8/200 => loss=1.4246, user_val_acc=0.7073, biz_val_acc=0.9227, ip_val_acc=0.6392\n",
      "Epoch 9/200 => loss=1.4183, user_val_acc=0.7068, biz_val_acc=0.9227, ip_val_acc=0.6442\n",
      "Epoch 10/200 => loss=1.4119, user_val_acc=0.7068, biz_val_acc=0.9227, ip_val_acc=0.6417\n",
      "Epoch 11/200 => loss=1.4053, user_val_acc=0.7068, biz_val_acc=0.9227, ip_val_acc=0.6392\n",
      "Epoch 12/200 => loss=1.3982, user_val_acc=0.7083, biz_val_acc=0.9227, ip_val_acc=0.6458\n",
      "Epoch 13/200 => loss=1.3906, user_val_acc=0.7087, biz_val_acc=0.9227, ip_val_acc=0.6475\n",
      "Epoch 14/200 => loss=1.3828, user_val_acc=0.7092, biz_val_acc=0.9227, ip_val_acc=0.6458\n",
      "Epoch 15/200 => loss=1.3748, user_val_acc=0.7081, biz_val_acc=0.9227, ip_val_acc=0.6458\n",
      "Epoch 16/200 => loss=1.3675, user_val_acc=0.7083, biz_val_acc=0.9227, ip_val_acc=0.6450\n",
      "Epoch 17/200 => loss=1.3606, user_val_acc=0.7081, biz_val_acc=0.9227, ip_val_acc=0.6475\n",
      "Epoch 18/200 => loss=1.3544, user_val_acc=0.7072, biz_val_acc=0.9227, ip_val_acc=0.6458\n",
      "Epoch 19/200 => loss=1.3485, user_val_acc=0.7095, biz_val_acc=0.9227, ip_val_acc=0.6458\n",
      "Epoch 20/200 => loss=1.3431, user_val_acc=0.7085, biz_val_acc=0.9227, ip_val_acc=0.6525\n",
      "Epoch 21/200 => loss=1.3381, user_val_acc=0.7097, biz_val_acc=0.9227, ip_val_acc=0.6517\n",
      "Epoch 22/200 => loss=1.3332, user_val_acc=0.7113, biz_val_acc=0.9227, ip_val_acc=0.6508\n",
      "Epoch 23/200 => loss=1.3287, user_val_acc=0.7111, biz_val_acc=0.9227, ip_val_acc=0.6483\n",
      "Epoch 24/200 => loss=1.3242, user_val_acc=0.7104, biz_val_acc=0.9227, ip_val_acc=0.6483\n",
      "Epoch 25/200 => loss=1.3199, user_val_acc=0.7099, biz_val_acc=0.9227, ip_val_acc=0.6442\n",
      "Epoch 26/200 => loss=1.3159, user_val_acc=0.7109, biz_val_acc=0.9227, ip_val_acc=0.6508\n",
      "Epoch 27/200 => loss=1.3120, user_val_acc=0.7105, biz_val_acc=0.9227, ip_val_acc=0.6542\n",
      "Epoch 28/200 => loss=1.3083, user_val_acc=0.7104, biz_val_acc=0.9227, ip_val_acc=0.6500\n",
      "Epoch 29/200 => loss=1.3048, user_val_acc=0.7097, biz_val_acc=0.9227, ip_val_acc=0.6417\n",
      "Epoch 30/200 => loss=1.3012, user_val_acc=0.7087, biz_val_acc=0.9227, ip_val_acc=0.6408\n",
      "Epoch 31/200 => loss=1.2979, user_val_acc=0.7113, biz_val_acc=0.9227, ip_val_acc=0.6467\n",
      "Epoch 32/200 => loss=1.2944, user_val_acc=0.7113, biz_val_acc=0.9227, ip_val_acc=0.6475\n",
      "Epoch 33/200 => loss=1.2911, user_val_acc=0.7107, biz_val_acc=0.9227, ip_val_acc=0.6450\n",
      "Epoch 34/200 => loss=1.2878, user_val_acc=0.7095, biz_val_acc=0.9227, ip_val_acc=0.6333\n",
      "Epoch 35/200 => loss=1.2845, user_val_acc=0.7100, biz_val_acc=0.9227, ip_val_acc=0.6358\n",
      "Epoch 36/200 => loss=1.2812, user_val_acc=0.7099, biz_val_acc=0.9227, ip_val_acc=0.6367\n",
      "Epoch 37/200 => loss=1.2779, user_val_acc=0.7099, biz_val_acc=0.9227, ip_val_acc=0.6392\n",
      "Epoch 38/200 => loss=1.2747, user_val_acc=0.7105, biz_val_acc=0.9227, ip_val_acc=0.6267\n",
      "Epoch 39/200 => loss=1.2714, user_val_acc=0.7105, biz_val_acc=0.9227, ip_val_acc=0.6242\n",
      "Epoch 40/200 => loss=1.2681, user_val_acc=0.7111, biz_val_acc=0.9227, ip_val_acc=0.6342\n",
      "Epoch 41/200 => loss=1.2645, user_val_acc=0.7111, biz_val_acc=0.9227, ip_val_acc=0.6225\n",
      "Epoch 42/200 => loss=1.2608, user_val_acc=0.7112, biz_val_acc=0.9227, ip_val_acc=0.6217\n",
      "Epoch 43/200 => loss=1.2573, user_val_acc=0.7113, biz_val_acc=0.9227, ip_val_acc=0.6275\n",
      "Epoch 44/200 => loss=1.2538, user_val_acc=0.7103, biz_val_acc=0.9227, ip_val_acc=0.6392\n",
      "Epoch 45/200 => loss=1.2504, user_val_acc=0.7125, biz_val_acc=0.9220, ip_val_acc=0.6267\n",
      "Epoch 46/200 => loss=1.2469, user_val_acc=0.7136, biz_val_acc=0.9227, ip_val_acc=0.6242\n",
      "Epoch 47/200 => loss=1.2439, user_val_acc=0.7127, biz_val_acc=0.9227, ip_val_acc=0.6283\n",
      "Epoch 48/200 => loss=1.2409, user_val_acc=0.7120, biz_val_acc=0.9227, ip_val_acc=0.6358\n",
      "Epoch 49/200 => loss=1.2380, user_val_acc=0.7132, biz_val_acc=0.9233, ip_val_acc=0.6208\n",
      "Epoch 50/200 => loss=1.2350, user_val_acc=0.7132, biz_val_acc=0.9240, ip_val_acc=0.6317\n",
      "Epoch 51/200 => loss=1.2324, user_val_acc=0.7124, biz_val_acc=0.9240, ip_val_acc=0.6233\n",
      "Epoch 52/200 => loss=1.2295, user_val_acc=0.7121, biz_val_acc=0.9233, ip_val_acc=0.6300\n",
      "Epoch 53/200 => loss=1.2270, user_val_acc=0.7113, biz_val_acc=0.9233, ip_val_acc=0.6308\n",
      "Epoch 54/200 => loss=1.2245, user_val_acc=0.7116, biz_val_acc=0.9233, ip_val_acc=0.6192\n",
      "Epoch 55/200 => loss=1.2220, user_val_acc=0.7129, biz_val_acc=0.9227, ip_val_acc=0.6183\n",
      "Epoch 56/200 => loss=1.2195, user_val_acc=0.7109, biz_val_acc=0.9227, ip_val_acc=0.6317\n",
      "Epoch 57/200 => loss=1.2172, user_val_acc=0.7147, biz_val_acc=0.9220, ip_val_acc=0.6208\n",
      "Epoch 58/200 => loss=1.2145, user_val_acc=0.7129, biz_val_acc=0.9220, ip_val_acc=0.6283\n",
      "Epoch 59/200 => loss=1.2118, user_val_acc=0.7123, biz_val_acc=0.9213, ip_val_acc=0.6300\n",
      "Epoch 60/200 => loss=1.2091, user_val_acc=0.7127, biz_val_acc=0.9200, ip_val_acc=0.6183\n",
      "Epoch 61/200 => loss=1.2066, user_val_acc=0.7121, biz_val_acc=0.9200, ip_val_acc=0.6275\n",
      "Epoch 62/200 => loss=1.2043, user_val_acc=0.7136, biz_val_acc=0.9207, ip_val_acc=0.6258\n",
      "Epoch 63/200 => loss=1.2018, user_val_acc=0.7133, biz_val_acc=0.9207, ip_val_acc=0.6250\n",
      "Epoch 64/200 => loss=1.1997, user_val_acc=0.7099, biz_val_acc=0.9207, ip_val_acc=0.6267\n",
      "Epoch 65/200 => loss=1.1974, user_val_acc=0.7131, biz_val_acc=0.9207, ip_val_acc=0.6267\n",
      "Epoch 66/200 => loss=1.1953, user_val_acc=0.7104, biz_val_acc=0.9207, ip_val_acc=0.6258\n",
      "Epoch 67/200 => loss=1.1933, user_val_acc=0.7131, biz_val_acc=0.9207, ip_val_acc=0.6250\n",
      "Epoch 68/200 => loss=1.1911, user_val_acc=0.7125, biz_val_acc=0.9207, ip_val_acc=0.6200\n",
      "Epoch 69/200 => loss=1.1892, user_val_acc=0.7104, biz_val_acc=0.9207, ip_val_acc=0.6283\n",
      "Epoch 70/200 => loss=1.1872, user_val_acc=0.7124, biz_val_acc=0.9207, ip_val_acc=0.6158\n",
      "Epoch 71/200 => loss=1.1854, user_val_acc=0.7120, biz_val_acc=0.9207, ip_val_acc=0.6250\n",
      "Epoch 72/200 => loss=1.1834, user_val_acc=0.7095, biz_val_acc=0.9207, ip_val_acc=0.6267\n",
      "Epoch 73/200 => loss=1.1816, user_val_acc=0.7121, biz_val_acc=0.9207, ip_val_acc=0.6142\n",
      "Epoch 74/200 => loss=1.1797, user_val_acc=0.7093, biz_val_acc=0.9207, ip_val_acc=0.6275\n",
      "Epoch 75/200 => loss=1.1777, user_val_acc=0.7095, biz_val_acc=0.9207, ip_val_acc=0.6258\n",
      "Epoch 76/200 => loss=1.1761, user_val_acc=0.7125, biz_val_acc=0.9207, ip_val_acc=0.6175\n",
      "Epoch 77/200 => loss=1.1740, user_val_acc=0.7116, biz_val_acc=0.9207, ip_val_acc=0.6217\n",
      "Epoch 78/200 => loss=1.1719, user_val_acc=0.7096, biz_val_acc=0.9200, ip_val_acc=0.6242\n",
      "Epoch 79/200 => loss=1.1702, user_val_acc=0.7119, biz_val_acc=0.9200, ip_val_acc=0.6158\n",
      "Epoch 80/200 => loss=1.1683, user_val_acc=0.7104, biz_val_acc=0.9213, ip_val_acc=0.6242\n",
      "Epoch 81/200 => loss=1.1665, user_val_acc=0.7120, biz_val_acc=0.9187, ip_val_acc=0.6142\n",
      "Epoch 82/200 => loss=1.1646, user_val_acc=0.7101, biz_val_acc=0.9207, ip_val_acc=0.6233\n",
      "Epoch 83/200 => loss=1.1627, user_val_acc=0.7117, biz_val_acc=0.9200, ip_val_acc=0.6183\n",
      "Epoch 84/200 => loss=1.1609, user_val_acc=0.7104, biz_val_acc=0.9207, ip_val_acc=0.6258\n",
      "Epoch 85/200 => loss=1.1591, user_val_acc=0.7131, biz_val_acc=0.9200, ip_val_acc=0.6175\n",
      "Epoch 86/200 => loss=1.1573, user_val_acc=0.7129, biz_val_acc=0.9207, ip_val_acc=0.6217\n",
      "Epoch 87/200 => loss=1.1556, user_val_acc=0.7115, biz_val_acc=0.9207, ip_val_acc=0.6217\n",
      "Epoch 88/200 => loss=1.1539, user_val_acc=0.7123, biz_val_acc=0.9207, ip_val_acc=0.6192\n",
      "Epoch 89/200 => loss=1.1524, user_val_acc=0.7096, biz_val_acc=0.9213, ip_val_acc=0.6258\n",
      "Epoch 90/200 => loss=1.1507, user_val_acc=0.7109, biz_val_acc=0.9213, ip_val_acc=0.6117\n",
      "Epoch 91/200 => loss=1.1492, user_val_acc=0.7115, biz_val_acc=0.9213, ip_val_acc=0.6175\n",
      "Epoch 92/200 => loss=1.1475, user_val_acc=0.7113, biz_val_acc=0.9207, ip_val_acc=0.6233\n",
      "Epoch 93/200 => loss=1.1459, user_val_acc=0.7120, biz_val_acc=0.9207, ip_val_acc=0.6158\n",
      "Epoch 94/200 => loss=1.1444, user_val_acc=0.7093, biz_val_acc=0.9207, ip_val_acc=0.6250\n",
      "Epoch 95/200 => loss=1.1429, user_val_acc=0.7115, biz_val_acc=0.9213, ip_val_acc=0.6175\n",
      "Epoch 96/200 => loss=1.1414, user_val_acc=0.7101, biz_val_acc=0.9207, ip_val_acc=0.6242\n",
      "Epoch 97/200 => loss=1.1398, user_val_acc=0.7099, biz_val_acc=0.9213, ip_val_acc=0.6200\n",
      "Epoch 98/200 => loss=1.1384, user_val_acc=0.7097, biz_val_acc=0.9220, ip_val_acc=0.6233\n",
      "Epoch 99/200 => loss=1.1371, user_val_acc=0.7101, biz_val_acc=0.9220, ip_val_acc=0.6075\n",
      "Epoch 100/200 => loss=1.1356, user_val_acc=0.7096, biz_val_acc=0.9220, ip_val_acc=0.6217\n",
      "Epoch 101/200 => loss=1.1341, user_val_acc=0.7087, biz_val_acc=0.9213, ip_val_acc=0.6100\n",
      "Epoch 102/200 => loss=1.1329, user_val_acc=0.7085, biz_val_acc=0.9213, ip_val_acc=0.6242\n",
      "Epoch 103/200 => loss=1.1316, user_val_acc=0.7096, biz_val_acc=0.9213, ip_val_acc=0.6125\n",
      "Epoch 104/200 => loss=1.1302, user_val_acc=0.7080, biz_val_acc=0.9213, ip_val_acc=0.6242\n",
      "Epoch 105/200 => loss=1.1287, user_val_acc=0.7088, biz_val_acc=0.9207, ip_val_acc=0.6142\n",
      "Epoch 106/200 => loss=1.1277, user_val_acc=0.7068, biz_val_acc=0.9213, ip_val_acc=0.6242\n",
      "Epoch 107/200 => loss=1.1260, user_val_acc=0.7100, biz_val_acc=0.9207, ip_val_acc=0.6133\n",
      "Epoch 108/200 => loss=1.1246, user_val_acc=0.7057, biz_val_acc=0.9207, ip_val_acc=0.6233\n",
      "Epoch 109/200 => loss=1.1233, user_val_acc=0.7084, biz_val_acc=0.9207, ip_val_acc=0.6133\n",
      "Epoch 110/200 => loss=1.1217, user_val_acc=0.7053, biz_val_acc=0.9200, ip_val_acc=0.6242\n",
      "Epoch 111/200 => loss=1.1207, user_val_acc=0.7081, biz_val_acc=0.9193, ip_val_acc=0.6108\n",
      "Epoch 112/200 => loss=1.1189, user_val_acc=0.7048, biz_val_acc=0.9200, ip_val_acc=0.6250\n",
      "Epoch 113/200 => loss=1.1175, user_val_acc=0.7075, biz_val_acc=0.9193, ip_val_acc=0.6133\n",
      "Epoch 114/200 => loss=1.1160, user_val_acc=0.7039, biz_val_acc=0.9200, ip_val_acc=0.6233\n",
      "Epoch 115/200 => loss=1.1147, user_val_acc=0.7077, biz_val_acc=0.9180, ip_val_acc=0.6058\n",
      "Epoch 116/200 => loss=1.1133, user_val_acc=0.7047, biz_val_acc=0.9193, ip_val_acc=0.6225\n",
      "Epoch 117/200 => loss=1.1120, user_val_acc=0.7072, biz_val_acc=0.9173, ip_val_acc=0.6042\n",
      "Epoch 118/200 => loss=1.1107, user_val_acc=0.7065, biz_val_acc=0.9187, ip_val_acc=0.6158\n",
      "Epoch 119/200 => loss=1.1093, user_val_acc=0.7036, biz_val_acc=0.9187, ip_val_acc=0.6242\n",
      "Epoch 120/200 => loss=1.1082, user_val_acc=0.7076, biz_val_acc=0.9173, ip_val_acc=0.6067\n",
      "Epoch 121/200 => loss=1.1071, user_val_acc=0.7049, biz_val_acc=0.9193, ip_val_acc=0.6233\n",
      "Epoch 122/200 => loss=1.1059, user_val_acc=0.7080, biz_val_acc=0.9173, ip_val_acc=0.6083\n",
      "Epoch 123/200 => loss=1.1046, user_val_acc=0.7059, biz_val_acc=0.9173, ip_val_acc=0.6242\n",
      "Epoch 124/200 => loss=1.1034, user_val_acc=0.7072, biz_val_acc=0.9173, ip_val_acc=0.6017\n",
      "Epoch 125/200 => loss=1.1024, user_val_acc=0.7048, biz_val_acc=0.9187, ip_val_acc=0.6208\n",
      "Epoch 126/200 => loss=1.1013, user_val_acc=0.7073, biz_val_acc=0.9173, ip_val_acc=0.6033\n",
      "Epoch 127/200 => loss=1.1001, user_val_acc=0.7053, biz_val_acc=0.9173, ip_val_acc=0.6233\n",
      "Epoch 128/200 => loss=1.0989, user_val_acc=0.7049, biz_val_acc=0.9173, ip_val_acc=0.6192\n",
      "Epoch 129/200 => loss=1.0980, user_val_acc=0.7052, biz_val_acc=0.9180, ip_val_acc=0.6075\n",
      "Epoch 130/200 => loss=1.0967, user_val_acc=0.7055, biz_val_acc=0.9173, ip_val_acc=0.6200\n",
      "Epoch 131/200 => loss=1.0958, user_val_acc=0.7071, biz_val_acc=0.9173, ip_val_acc=0.5992\n",
      "Epoch 132/200 => loss=1.0947, user_val_acc=0.7057, biz_val_acc=0.9193, ip_val_acc=0.6250\n",
      "Epoch 133/200 => loss=1.0935, user_val_acc=0.7055, biz_val_acc=0.9167, ip_val_acc=0.5975\n",
      "Epoch 134/200 => loss=1.0928, user_val_acc=0.7060, biz_val_acc=0.9180, ip_val_acc=0.6208\n",
      "Epoch 135/200 => loss=1.0915, user_val_acc=0.7035, biz_val_acc=0.9167, ip_val_acc=0.6042\n",
      "Epoch 136/200 => loss=1.0904, user_val_acc=0.7040, biz_val_acc=0.9193, ip_val_acc=0.6058\n",
      "Epoch 137/200 => loss=1.0895, user_val_acc=0.7031, biz_val_acc=0.9167, ip_val_acc=0.6008\n",
      "Epoch 138/200 => loss=1.0883, user_val_acc=0.7051, biz_val_acc=0.9167, ip_val_acc=0.6200\n",
      "Epoch 139/200 => loss=1.0875, user_val_acc=0.7051, biz_val_acc=0.9160, ip_val_acc=0.5983\n",
      "Epoch 140/200 => loss=1.0864, user_val_acc=0.7045, biz_val_acc=0.9160, ip_val_acc=0.5925\n",
      "Epoch 141/200 => loss=1.0855, user_val_acc=0.7044, biz_val_acc=0.9167, ip_val_acc=0.5967\n",
      "Epoch 142/200 => loss=1.0842, user_val_acc=0.7032, biz_val_acc=0.9173, ip_val_acc=0.6100\n",
      "Epoch 143/200 => loss=1.0834, user_val_acc=0.7039, biz_val_acc=0.9173, ip_val_acc=0.5975\n",
      "Epoch 144/200 => loss=1.0825, user_val_acc=0.7032, biz_val_acc=0.9173, ip_val_acc=0.6083\n",
      "Epoch 145/200 => loss=1.0816, user_val_acc=0.7033, biz_val_acc=0.9173, ip_val_acc=0.6067\n",
      "Epoch 146/200 => loss=1.0805, user_val_acc=0.7051, biz_val_acc=0.9147, ip_val_acc=0.5892\n",
      "Epoch 147/200 => loss=1.0799, user_val_acc=0.7024, biz_val_acc=0.9180, ip_val_acc=0.6125\n",
      "Epoch 148/200 => loss=1.0788, user_val_acc=0.7048, biz_val_acc=0.9173, ip_val_acc=0.5900\n",
      "Epoch 149/200 => loss=1.0776, user_val_acc=0.7039, biz_val_acc=0.9180, ip_val_acc=0.5933\n",
      "Epoch 150/200 => loss=1.0773, user_val_acc=0.7048, biz_val_acc=0.9173, ip_val_acc=0.6042\n",
      "Epoch 151/200 => loss=1.0761, user_val_acc=0.7039, biz_val_acc=0.9180, ip_val_acc=0.6125\n",
      "Epoch 152/200 => loss=1.0754, user_val_acc=0.7053, biz_val_acc=0.9140, ip_val_acc=0.5875\n",
      "Epoch 153/200 => loss=1.0743, user_val_acc=0.7048, biz_val_acc=0.9180, ip_val_acc=0.6150\n",
      "Epoch 154/200 => loss=1.0735, user_val_acc=0.7035, biz_val_acc=0.9153, ip_val_acc=0.5875\n",
      "Epoch 155/200 => loss=1.0728, user_val_acc=0.7043, biz_val_acc=0.9187, ip_val_acc=0.6117\n",
      "Epoch 156/200 => loss=1.0716, user_val_acc=0.7043, biz_val_acc=0.9153, ip_val_acc=0.5858\n",
      "Epoch 157/200 => loss=1.0709, user_val_acc=0.7041, biz_val_acc=0.9160, ip_val_acc=0.6067\n",
      "Epoch 158/200 => loss=1.0702, user_val_acc=0.7039, biz_val_acc=0.9153, ip_val_acc=0.5883\n",
      "Epoch 159/200 => loss=1.0691, user_val_acc=0.7057, biz_val_acc=0.9180, ip_val_acc=0.6008\n",
      "Epoch 160/200 => loss=1.0683, user_val_acc=0.7031, biz_val_acc=0.9173, ip_val_acc=0.5975\n",
      "Epoch 161/200 => loss=1.0678, user_val_acc=0.7053, biz_val_acc=0.9160, ip_val_acc=0.5908\n",
      "Epoch 162/200 => loss=1.0667, user_val_acc=0.7015, biz_val_acc=0.9187, ip_val_acc=0.6117\n",
      "Epoch 163/200 => loss=1.0656, user_val_acc=0.7029, biz_val_acc=0.9160, ip_val_acc=0.5867\n",
      "Epoch 164/200 => loss=1.0651, user_val_acc=0.7045, biz_val_acc=0.9167, ip_val_acc=0.6117\n",
      "Epoch 165/200 => loss=1.0643, user_val_acc=0.7037, biz_val_acc=0.9167, ip_val_acc=0.5842\n",
      "Epoch 166/200 => loss=1.0634, user_val_acc=0.7051, biz_val_acc=0.9173, ip_val_acc=0.5925\n",
      "Epoch 167/200 => loss=1.0629, user_val_acc=0.7047, biz_val_acc=0.9160, ip_val_acc=0.5842\n",
      "Epoch 168/200 => loss=1.0617, user_val_acc=0.7039, biz_val_acc=0.9180, ip_val_acc=0.6100\n",
      "Epoch 169/200 => loss=1.0612, user_val_acc=0.7033, biz_val_acc=0.9167, ip_val_acc=0.5867\n",
      "Epoch 170/200 => loss=1.0603, user_val_acc=0.7056, biz_val_acc=0.9173, ip_val_acc=0.5908\n",
      "Epoch 171/200 => loss=1.0596, user_val_acc=0.7036, biz_val_acc=0.9173, ip_val_acc=0.6075\n",
      "Epoch 172/200 => loss=1.0587, user_val_acc=0.7021, biz_val_acc=0.9160, ip_val_acc=0.5833\n",
      "Epoch 173/200 => loss=1.0581, user_val_acc=0.7044, biz_val_acc=0.9173, ip_val_acc=0.5983\n",
      "Epoch 174/200 => loss=1.0574, user_val_acc=0.7036, biz_val_acc=0.9173, ip_val_acc=0.5975\n",
      "Epoch 175/200 => loss=1.0563, user_val_acc=0.7036, biz_val_acc=0.9160, ip_val_acc=0.5817\n",
      "Epoch 176/200 => loss=1.0563, user_val_acc=0.7029, biz_val_acc=0.9173, ip_val_acc=0.6017\n",
      "Epoch 177/200 => loss=1.0552, user_val_acc=0.7052, biz_val_acc=0.9167, ip_val_acc=0.5875\n",
      "Epoch 178/200 => loss=1.0544, user_val_acc=0.7051, biz_val_acc=0.9173, ip_val_acc=0.5983\n",
      "Epoch 179/200 => loss=1.0537, user_val_acc=0.7053, biz_val_acc=0.9173, ip_val_acc=0.5983\n",
      "Epoch 180/200 => loss=1.0527, user_val_acc=0.7043, biz_val_acc=0.9167, ip_val_acc=0.5775\n",
      "Epoch 181/200 => loss=1.0519, user_val_acc=0.7032, biz_val_acc=0.9167, ip_val_acc=0.5917\n",
      "Epoch 182/200 => loss=1.0515, user_val_acc=0.7040, biz_val_acc=0.9167, ip_val_acc=0.5800\n",
      "Epoch 183/200 => loss=1.0504, user_val_acc=0.7031, biz_val_acc=0.9167, ip_val_acc=0.5975\n",
      "Epoch 184/200 => loss=1.0499, user_val_acc=0.7039, biz_val_acc=0.9167, ip_val_acc=0.5883\n",
      "Epoch 185/200 => loss=1.0488, user_val_acc=0.7027, biz_val_acc=0.9173, ip_val_acc=0.5767\n",
      "Epoch 186/200 => loss=1.0481, user_val_acc=0.7020, biz_val_acc=0.9173, ip_val_acc=0.5875\n",
      "Epoch 187/200 => loss=1.0477, user_val_acc=0.7043, biz_val_acc=0.9160, ip_val_acc=0.5850\n",
      "Epoch 188/200 => loss=1.0465, user_val_acc=0.7035, biz_val_acc=0.9160, ip_val_acc=0.5817\n",
      "Epoch 189/200 => loss=1.0461, user_val_acc=0.7041, biz_val_acc=0.9187, ip_val_acc=0.6100\n",
      "Epoch 190/200 => loss=1.0451, user_val_acc=0.7028, biz_val_acc=0.9153, ip_val_acc=0.5800\n",
      "Epoch 191/200 => loss=1.0443, user_val_acc=0.7041, biz_val_acc=0.9153, ip_val_acc=0.5858\n",
      "Epoch 192/200 => loss=1.0437, user_val_acc=0.7041, biz_val_acc=0.9153, ip_val_acc=0.5867\n",
      "Epoch 193/200 => loss=1.0428, user_val_acc=0.7016, biz_val_acc=0.9147, ip_val_acc=0.5725\n",
      "Epoch 194/200 => loss=1.0423, user_val_acc=0.7044, biz_val_acc=0.9173, ip_val_acc=0.6008\n",
      "Epoch 195/200 => loss=1.0413, user_val_acc=0.7019, biz_val_acc=0.9147, ip_val_acc=0.5792\n",
      "Epoch 196/200 => loss=1.0408, user_val_acc=0.7036, biz_val_acc=0.9167, ip_val_acc=0.5867\n",
      "Epoch 197/200 => loss=1.0404, user_val_acc=0.7040, biz_val_acc=0.9147, ip_val_acc=0.5925\n",
      "Epoch 198/200 => loss=1.0394, user_val_acc=0.7029, biz_val_acc=0.9167, ip_val_acc=0.5858\n",
      "Epoch 199/200 => loss=1.0386, user_val_acc=0.7017, biz_val_acc=0.9140, ip_val_acc=0.5750\n",
      "Epoch 200/200 => loss=1.0381, user_val_acc=0.7065, biz_val_acc=0.9160, ip_val_acc=0.5967\n",
      "\n",
      "Final Evaluations:\n",
      "User Test Accuracy: 0.6944\n",
      "Business Test Accuracy: 0.9187\n",
      "IP Test Accuracy: 0.6150\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Cell 8: Main Training Loop\n",
    "# --------------------------------------------------------------------------------\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    avg_loss = 0.0\n",
    "    for step in range(STEPS_PER_EPOCH):\n",
    "        # If truly mini-batching, you'd do subgraph sampling here.\n",
    "        loss_val = train_step()\n",
    "        avg_loss += loss_val\n",
    "\n",
    "    if STEPS_PER_EPOCH > 1:\n",
    "        avg_loss /= STEPS_PER_EPOCH\n",
    "\n",
    "    # Evaluate on validation masks\n",
    "    val_acc_user = evaluate_user('val_mask')\n",
    "    val_acc_biz  = evaluate_biz('val_mask') if MULTI_TASK else 0.0\n",
    "    val_acc_ip   = evaluate_ip('val_mask') if IP_CLASSIFICATION else 0.0\n",
    "\n",
    "    if epoch % PRINT_EVERY == 0:\n",
    "        print(f\"Epoch {epoch}/{EPOCHS} => loss={avg_loss:.4f}, \"\n",
    "              f\"user_val_acc={val_acc_user:.4f}, biz_val_acc={val_acc_biz:.4f}, ip_val_acc={val_acc_ip:.4f}\")\n",
    "\n",
    "# Final test evaluation\n",
    "print(\"\\nFinal Evaluations:\")\n",
    "user_test_acc = evaluate_user('test_mask')\n",
    "print(f\"User Test Accuracy: {user_test_acc:.4f}\")\n",
    "if MULTI_TASK:\n",
    "    biz_test_acc = evaluate_biz('test_mask')\n",
    "    print(f\"Business Test Accuracy: {biz_test_acc:.4f}\")\n",
    "if IP_CLASSIFICATION:\n",
    "    ip_test_acc = evaluate_ip('test_mask')\n",
    "    print(f\"IP Test Accuracy: {ip_test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done!\n",
      "We've built a multi-edge, multi-task GNN using PyTorch Geometric:\n",
      "- Node types: user, business, ip\n",
      "- Edge types: user->user, user->business, user->ip (+ reversed edges)\n",
      "- Multi-task heads for user, business, and IP classification\n",
      "- 'steps_per_epoch' formula demonstrated for mini-batch. \n",
      "- Final accuracy results printed for each node type.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Cell 9: Wrap-Up\n",
    "# --------------------------------------------------------------------------------\n",
    "print(\"\"\"\n",
    "Done!\n",
    "We've built a multi-edge, multi-task GNN using PyTorch Geometric:\n",
    "- Node types: user, business, ip\n",
    "- Edge types: user->user, user->business, user->ip (+ reversed edges)\n",
    "- Multi-task heads for user, business, and IP classification\n",
    "- 'steps_per_epoch' formula demonstrated for mini-batch. \n",
    "- Final accuracy results printed for each node type.\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
